{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb1cf8a",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "This part contains the extraction and cleaning of data from three Twitter news accounts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b240d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063aadd",
   "metadata": {},
   "source": [
    "### Extracting tweets from Twitter API using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAMwJWwEAAAAA9fboJE9mqyS5sH90WY6rfAFBvPw%3Dw3wtOF3C77Os6E25qJJzp1n16PbFt864V6B4vWC6OCsh02ARct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbfa294",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_breaking_news_id = '428333'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_breaking_news_id = '5402612'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384abbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_id = '1652541'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6d8dc",
   "metadata": {},
   "source": [
    "#### Use paginator to retrieve the 3200 tweets for the account\n",
    "Populate the tweet and the information collected into a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd108cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cnn_tweets = tweepy.Paginator(client.get_users_tweets, id=cnn_breaking_news_id, max_results=100, tweet_fields=['context_annotations','created_at','public_metrics']).flatten(limit=3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880c345c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e95722faecdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbbc_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaginator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_users_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbc_breaking_news_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context_annotations'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'public_metrics'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "bbc_tweets = tweepy.Paginator(client.get_users_tweets, id=bbc_breaking_news_id, max_results=100, tweet_fields=['context_annotations','created_at','public_metrics']).flatten(limit=3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e29fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7956fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_tweets = tweepy.Paginator(client.get_users_tweets, id=reuters_id, max_results=100, tweet_fields=['context_annotations','created_at','public_metrics']).flatten(limit=3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8eb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_all_tweets = []\n",
    "for tweet in cnn_tweets:\n",
    "    cnn_all_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc99523",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_all_tweets = []\n",
    "for tweet in bbc_tweets:\n",
    "    bbc_all_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d81c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_all_tweets = []\n",
    "for tweet in reu_tweets:\n",
    "    reu_all_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f2db0",
   "metadata": {},
   "source": [
    "#### Create dataframe that saves the information of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ef9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates dataframe that takes in the list of all tweets collected using the Tweepy Paginator. \n",
    "\n",
    "It has columns that contain different information from the tweet, including\n",
    "\"tweet text\" - the text of the Tweet\n",
    "'# likes' - # of likes the Tweet had (at the time of retrieval)\n",
    "'# retweets' - # of retweets the Tweet had (at the time of retrieval)\n",
    "'# replies' - # of replies the Tweet had (at the time of retrieval)\n",
    "'# quotes' - # of quote retweets the Tweet had (at the time of retrieval)\n",
    "'links' - the hyperlink of the tweet that links to the full news article\n",
    "\"\"\"\n",
    "\n",
    "def create_df(all_tweets):\n",
    "    df = pd.DataFrame(all_tweets)\n",
    "    \n",
    "    #create new column\n",
    "    df[\"tweet text\"] = \"\"\n",
    "    df[\"# likes\"] = \"\"\n",
    "    df[\"# retweets\"] = \"\"\n",
    "    df[\"# replies\"] = \"\"\n",
    "    df[\"# quotes\"] = \"\"\n",
    "    df[\"links\"] = \"\"\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        df.iloc[index,5] = df.iloc[index,4].replace('\\n','').split(\"http\")[0] #tweet text - everything before the href link\n",
    "        df.iloc[index,6] = df.iloc[index,3][\"like_count\"] # like count\n",
    "        df.iloc[index,7] = df.iloc[index,3][\"retweet_count\"] # retweets count\n",
    "        df.iloc[index,8] = df.iloc[index,3][\"reply_count\"] # reply count\n",
    "        df.iloc[index,9] = df.iloc[index,3][\"quote_count\"] # quote count\n",
    "        \n",
    "    df[\"# Reactions\"] = df[\"# likes\"] + df[\"# retweets\"] + df[\"# replies\"] + df[\"# quotes\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df = create_df(cnn_all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb129f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df = create_df(bbc_all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_df = create_df(reu_all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f88f4a",
   "metadata": {},
   "source": [
    "### After every section, save the newest dataframe to csv to ensure that files are being saved, so we don't need to run the code again all the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df.to_csv(\"CNN Breaking New.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df.to_csv(\"BBC Breaking News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da098c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_df.to_csv(\"Reuters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d19ac",
   "metadata": {},
   "source": [
    "### Open Saved CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df = pd.read_csv(\"CNN Breaking News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfdf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df = pd.read_csv(\"BBC Breaking News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505217c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_df = pd.read_csv(\"Reuters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88a720",
   "metadata": {},
   "source": [
    "### Extract the URL Links from the tweet text\n",
    "The news tweet text contains two parts: <br> 1) The tweet text itself (string format) <br> 2) A hyperlink (in string format) that takes you to the full article of the news <br> To extract the hyperlink from the tweet text, we have to use the urlextract library that can find the hyperlink directly. We collected all of the links and add it to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da608d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df = bbc_df.drop(columns = ['Unnamed: 0']) #drop colummn that exist when importing csv\n",
    "reu_df = reu_df.drop(columns = ['Unnamed: 0']) \n",
    "cnn_df = reu_df.drop(columns = ['Unnamed: 0']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function extract the url from the tweet text, from an input of dataframe\n",
    "This doesn't work if the tweet text is a retweet, as it won't trace back to the original tweet. \n",
    "But can still collect countries mentioned in the text.\"\"\"\n",
    "\n",
    "def extractURL(dataframe):\n",
    "    #try and except for errors that can occur, such as when no links are mentioned in the tweet\n",
    "    try:\n",
    "        for index, row in dataframe.iterrows():\n",
    "            text = dataframe.at[index,\"tweet text\"]\n",
    "            extractor = URLExtract()\n",
    "            url = extractor.find_urls(text)\n",
    "            dataframe.at[index,\"links\"] = url\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This functions is unique for the tweets from Retuers. \n",
    "For the other two news account, their tweets always end with the hyperlink. \n",
    "However, Reuters' format for tweet is different, it sometimes contain an emoji after the hyperlink, \n",
    "therefore we have to break it down.\n",
    "\n",
    "Extract the url from the tweet text\n",
    "Doesn't work if the tweet text is a retweet, as it won't trace back to the original tweet. \n",
    "But can still collect countries mentioned in the text.\"\"\"\n",
    "\n",
    "def extractREU_URL(dataframe):\n",
    "    #try and except for errors that can occur, such as when no links are mentioned in the tweet\n",
    "    try:\n",
    "        for index, row in dataframe.iterrows():\n",
    "            text = dataframe.at[index,\"tweet text\"]\n",
    "            extractor = URLExtract()\n",
    "            url = extractor.find_urls(text)\n",
    "\n",
    "            if len(url) > 1:\n",
    "                dataframe.at[index,\"links\"] = url[0]\n",
    "            else:\n",
    "                dataframe.at[index,\"links\"] = str(url)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb50307",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractURL(bbc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c873db",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractURL(cnn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cd4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractREU_URL(reu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df.to_csv(\"CNN Breaking News.csv\") #save new csv file after every section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df.to_csv(\"BBC Breaking News.csv\") #save new csv file after every section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_df.to_csv(\"Reuters.csv\") #save new csv file after every section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14194a2",
   "metadata": {},
   "source": [
    "### Extract countries from the text\n",
    "\n",
    "**Part 1**: After collecting the text of the tweet using Twitter API, we have to find the countries mentioned in the tweet. For the first part, we only look at the geographic locations mentioned in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e96cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open Saved CSV Files\n",
    "cnn_df = pd.read_csv(\"CNN Breaking News.csv\")\n",
    "bbc_df = pd.read_csv(\"BBC Breaking News.csv\")\n",
    "reu_df = pd.read_csv(\"Reuters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use spacy libray, that uses natural language processing to find geographic location\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9ef66",
   "metadata": {},
   "source": [
    "#### Example for spacy \n",
    "If we parse the text _\"United States, Bethoven, Science, BBC, dogs, Bella\"_ through nlp, \n",
    "it would return  \n",
    ">    [('United States', 0, 13, 'GPE'),  \n",
    "     ('Bethoven', 15, 23, 'GPE'),  \n",
    "     ('Science', 25, 32, 'ORG'),  \n",
    "     ('BBC', 34, 37, 'ORG'),  \n",
    "     ('Bella', 45, 50, 'PERSON')]\n",
    "     \n",
    "The array includes the phrase, start_char, end_char, and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df[\"Countries from text\"] = \"\" #create new column to populate later\n",
    "bbc_df[\"Countries from text\"] = \"\"\n",
    "reu_df[\"Countries from text\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d19641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes in the dataframe, looks at the text collected previously, \n",
    "and return a new dataframe with places mentioned in the tweet text\n",
    "\n",
    "It uses the library spacy that retrieve information of all the words, using natural language processing\n",
    "\n",
    "\"\"\"\n",
    "def find_places(dataframe):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        text = dataframe.at[index,\"tweet text\"] #tweet text\n",
    "        \n",
    "        #try and except to handle error\n",
    "        try:\n",
    "            text = nlp(text)\n",
    "            words = []\n",
    "            \n",
    "            #retrieve information of all the words in the text\n",
    "            for ent in text.ents:\n",
    "                ent_words = ent.text, ent.start_char, ent.end_char, ent.label_\n",
    "                words.append(ent_words)\n",
    "                \n",
    "            final_places = []\n",
    "            \n",
    "            for word in words: \n",
    "                if word[3] == \"GPE\": #the third part gives the label (GPE) of the word/words\n",
    "                    place = word[0] #the first part returns the actual word/words\n",
    "                    \n",
    "                    #clean the name of the place to a format that is recognizable for the next step\n",
    "                    if \"'\" in place:\n",
    "                        place = place.split(\"'\")[0]\n",
    "                    if \"the\" in place: #we don't want locations like \"The United States\"\n",
    "                        place = place.split(\"the \")[-1]\n",
    "                    if \"province\" in place: #we don't want locations like \"Zhejiang province\"\n",
    "                        place = place.replace(\"province\", \"\") \n",
    "                        \n",
    "                    final_places.append(place) #add places into the final list\n",
    "\n",
    "            dataframe.at[index,\"Countries from text\"] = final_places #populate column\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_places(cnn_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b308d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_places(bbc_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba386f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_places(reu_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df.to_csv(\"CNN Breaking News.csv\") #save new csv file after every section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df.to_csv(\"BBC Breaking News.csv\") #save new csv file after every section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_df.to_csv(\"Reuters.csv\") #save new csv file after every section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0af0c6",
   "metadata": {},
   "source": [
    "### Extract the news article text from the URL Links via web scraping for CNN\n",
    "**Part 2**: Extract the text from the news article that was in a hyperlink from the Tweet. <br>\n",
    "Since each news website have different format for their article, we have to inspect each of the website.\n",
    "\n",
    "This doesn't work under a few circumstances: <br>\n",
    "1. Tweets that are retweeted, we cannot trace back to the original link \n",
    "2. \"Live Update\" on CNN Breaking News as there are no hyperlink for these tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c981b",
   "metadata": {},
   "source": [
    "Step 1: Access the URL text via link using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTMLPage(url):\n",
    "    \"\"\"Given a url, get the HTML page content\"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(\"Error: {}. Failure resaon: {}\".format(response.status_code, \n",
    "                                                     response.reason))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This functions takes in the url from CNN tweets and gets the text of the article from the CNN Website\"\"\"\n",
    "\n",
    "def getURLText_CNN(url):\n",
    "    \n",
    "    page = getHTMLPage(url)\n",
    "    if page == None:\n",
    "        return \"\"\n",
    "    \n",
    "    domTree = BS(page, 'html.parser')\n",
    "    first = domTree.find_all('p') #find first paragraph as it's under the format 'p'\n",
    "    divs = domTree.find_all('div',{'class':'zn-body__paragraph'}) #find remaining paragraphs\n",
    "    paragraphs = []\n",
    "    \n",
    "    #save only the first paragraph from all 'p'\n",
    "    for fir in first:\n",
    "        if \"(CNN\" in fir.text:\n",
    "            paragraphs.append(fir.text) #only want the text\n",
    "\n",
    "    for div in divs:\n",
    "        paragraphs.append(div.text)\n",
    "        \n",
    "    concatenated_para = \"\".join(paragraphs)\n",
    "    return concatenated_para\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use getURLText_CNN to find the concatenated paragraphs and save them to a text file\"\"\"\n",
    "\n",
    "for index, row in cnn_df.iterrows():\n",
    "    ID = cnn_df.at[index,\"id\"]\n",
    "    directory = \"/Users/jennychan/Desktop/CS 234 Final Project/CNN Breaking News/\" #set file path\n",
    "    name = \"{}{}.txt\".format(directory, ID) #save name as tweetid\n",
    "    url = cnn_df.at[index,\"links\"]\n",
    "    \n",
    "    if url == None: #if there are no hyperlink in the dataframe\n",
    "        with open(name, 'w') as f: \n",
    "            f.write(\"None\")\n",
    "    else:\n",
    "        text = getURLText_CNN(url) #else, get the text from the hyperlink\n",
    "        with open(name, 'w') as f: \n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb73207",
   "metadata": {},
   "source": [
    "### Extract the news article text from the URL Links via web scraping for BBC\n",
    "**Part 2**: Extract the text from the news article that was in a hyperlink from the Tweet. <br>\n",
    "Since each news website have different format for their article, we have to inspect each of the website.\n",
    "\n",
    "This doesn't work under a few circumstances: <br>\n",
    "1. Tweets that are retweeted, we cannot trace back to the original link \n",
    "2. \"Live Update\" on BBC Breaking News as there are no hyperlink for these tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This functions takes in the url from BBC tweets and gets the text of the article from the BBC Website\"\"\"\n",
    "\n",
    "def getURLText_BBC(url):\n",
    "    \n",
    "    page = getHTMLPage(url)\n",
    "    if page == None:\n",
    "        print(\"\") #contains broken link\n",
    "        \n",
    "    try:\n",
    "        domTree = BS(page, 'html.parser')\n",
    "        ps = domTree.find_all('p',{'class':'ssrcss-1q0x1qg-Paragraph eq5iqo00'}) #under this class in 'p'\n",
    "        paragraphs = []\n",
    "        \n",
    "        for p in ps:\n",
    "            paragraphs.append(p.text) \n",
    "            \n",
    "        concatenated_para = \"\".join(paragraphs) #joins all paragraph togehter\n",
    "        \n",
    "        return concatenated_para\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use getURLText_BBC to find the concatenated paragraphs and save them to a text file\"\"\"\n",
    "\n",
    "for index in range(0,len(bbc_df)) : \n",
    "    ID = bbc_df.at[index,\"id\"]\n",
    "    directory = \"/Users/jennychan/Desktop/CS 234 Final Project/BBC Breaking News/\" #set file path\n",
    "    name = \"{}{}.txt\".format(directory, ID)\n",
    "    url = bbc_df.at[index,\"links\"]\n",
    "    \n",
    "    if pd.isna(url): #if there are no hyperlink in the dataframe\n",
    "        with open(name, 'w') as f: \n",
    "            f.write(\"None\")\n",
    "    else:\n",
    "        try:\n",
    "            text = getURLText_BBC(url) #else, get the text from the hyperlink\n",
    "            with open(name, 'w') as f: \n",
    "                f.write(text)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5a4df",
   "metadata": {},
   "source": [
    "### Extract the news article text from the URL Links via web scraping for Reuters\n",
    "**Part 2**: Extract the text from the news article that was in a hyperlink from the Tweet. <br>\n",
    "Since each news website have different format for their article, we have to inspect each of the website.\n",
    "\n",
    "This doesn't work under a few circumstances: <br>\n",
    "1. Tweets that are retweeted, we cannot trace back to the original link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6119c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURLText_Reu(url):\n",
    "    \n",
    "    page = getHTMLPage(url)\n",
    "    \n",
    "    if page == None: #broken link\n",
    "        print(\"\")\n",
    "    try:\n",
    "        domTree = BS(page, 'html.parser')\n",
    "        \n",
    "        #paragraphs are under this class under 'p'\n",
    "        ps = domTree.find_all('p',{'class':'Text__text___3eVx1j Text__dark-grey___AS2I_p Text__regular___Bh17t- Text__large___1i0u1F Body__base___25kqPt Body__large_body___3g04wK ArticleBody__element___3UrnEs'})\n",
    "        paragraphs = []\n",
    "        for p in ps:\n",
    "            paragraphs.append(p.text)\n",
    "        concatenated_para = \" \".join(paragraphs)\n",
    "        return concatenated_para\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a92906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use getURLText_Reu to find the concatenated paragraphs and save them to a text file\"\"\"\n",
    "\n",
    "for index in range(0,len(reu_df)) : \n",
    "    ID = reu_df.at[index,\"id\"]\n",
    "    directory = \"/Users/jennychan/Desktop/CS 234 Final Project/Reuters/\" #set file path\n",
    "    name = \"{}{}.txt\".format(directory, ID)\n",
    "    url = reu_df.at[index,\"links\"]\n",
    "    \n",
    "    if pd.isna(url): #if there are no hyperlink in the dataframe\n",
    "        with open(name, 'w') as f: \n",
    "            f.write(\"None\")\n",
    "    else:\n",
    "        try:\n",
    "            text = getURLText_Reu(url) #else, get the text from the hyperlink\n",
    "            with open(name, 'w') as f: \n",
    "                f.write(text)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831b643",
   "metadata": {},
   "source": [
    "### Extract country/countries mentioned from the news article in the save text file\n",
    "If the tweet didn't mention any countries in the text, we check for the countries mentioned in the news article that we have saved. People would tend tend to look at the tweet itself first, and react based on it. Therefore, we would save the countries mentioned in the tweet and wouldn't look into the article if there are already countries mentioned in the tweet. Otherwise, we look at the article and look for countries mentioned. Sometimes, the tweet might be also about a country/place, but the nlp didn't pick it up, so we would look into the article for further context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028de3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for final countries\n",
    "cnn_df[\"Final Countries\"] = \"\"\n",
    "bbc_df[\"Final Countries\"] = \"\"\n",
    "reu_df[\"Final Countries\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes in all the countries mentioned (can repeat) in the text that was stored in a list, \n",
    "it uses to return a list that contains the two countries that are most frequently in the text.\n",
    "\n",
    "\"\"\"\n",
    "def top_two_places(all_countries):\n",
    "    two = []\n",
    "    count = Counter(all_countries)\n",
    "    top_countries = count.most_common(2) #two most frequently mentioned countries, in a counter format\n",
    "    \n",
    "    for country in top_countries: \n",
    "        two.append(country[0])\n",
    "    return two #return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aaf379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function uses the library geopy.\n",
    "It takes in a city/state/location mentioned and returns the country of the location\n",
    "\n",
    "\"\"\"\n",
    "def country_from_city(place):\n",
    "    \n",
    "    try:\n",
    "        geolocator = Nominatim(user_agent = \"geoapiExercises\") #initiate the geolocator that looks for the country using the 'geoapi'\n",
    "        location = geolocator.geocode(place, language=\"en\") #language is english, or else it returns country names in their local language\n",
    "        full_address = location.address #gives the full address of the place (including city, states, provinces etc)\n",
    "        final_country = full_address.split(\",\")[-1].strip() #only want the last part, which is the country\n",
    "        return final_country\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a716015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract Countries for all tweet based on the text or from the article in the hyperlink\"\"\"\n",
    "\n",
    "directory = \"/Users/jennychan/Desktop/CS 234 Final Project/CNN Breaking News/\"\n",
    "\n",
    "for index in range(0,len(cnn_df)) :\n",
    "    tweet_countries = cnn_df.at[index,\"Countries from text\"]\n",
    "    \n",
    "    if len(tweet_countries) != 0: # if tweet text contains countries already\n",
    "        countries_mentioned = []\n",
    "        for place in tweet_countries:\n",
    "            country = country_from_city(place)\n",
    "    \n",
    "            if country not in countries_mentioned: #don't double add\n",
    "                countries_mentioned.append(country)\n",
    "            else:\n",
    "                pass\n",
    "                    \n",
    "        bbc_df.at[index,\"Final Countries\"] = countries_mentioned\n",
    "    \n",
    "    else: #if they didn't mention countries in the text, look into the txt file\n",
    "        tweetID = df.at[index,\"id\"]\n",
    "        with open('{}{}.txt'.format(directory, tweetID)) as f: #open the article\n",
    "            lines = f.read()\n",
    "            all_places = find_places(lines)\n",
    "            top2 = top_two_places(all_places)\n",
    "            countries_mentioned = []\n",
    "\n",
    "            if len(top2) != 0:\n",
    "                for place in top2:\n",
    "                    if place == \"NoneType\": # no countries mentioned\n",
    "                        pass\n",
    "                    country = country_from_city(place)\n",
    "\n",
    "                    if country not in countries_mentioned: #don't double add\n",
    "                        countries_mentioned.append(country)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        df.at[index,\"Final Countries\"] = countries_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Same as above, but for BBC\n",
    "Extract Countries for all tweet based on the text or from the article in the hyperlink or the tweet text\"\"\"\n",
    "\n",
    "bbc_directory = \"/Users/jennychan/Desktop/CS 234 Final Project/BBC Breaking News/\"\n",
    "\n",
    "for index in range(0,len(bbc_df)) :\n",
    "    tweet_countries = bbc_df.at[index,\"Countries from text\"]\n",
    "    \n",
    "    if len(tweet_countries) != 0: # if tweet text contains countries already\n",
    "        countries_mentioned = []\n",
    "        for place in tweet_countries:\n",
    "            country = country_from_city(place)\n",
    "    \n",
    "            if country not in countries_mentioned: #don't double add\n",
    "                countries_mentioned.append(country)\n",
    "            else:\n",
    "                pass\n",
    "                    \n",
    "        bbc_df.at[index,\"Final Countries\"] = countries_mentioned\n",
    "    \n",
    "    else: #if they didn't mention countries in the text, look into the txt file\n",
    "        tweetID = bbc_df.at[index,\"id\"]\n",
    "        with open('{}{}.txt'.format(bbc_directory, tweetID)) as f:\n",
    "            lines = f.read()\n",
    "            all_places = find_places(bbc_df)\n",
    "            top2 = top_two_places(all_places)\n",
    "            countries_mentioned = []\n",
    "            \n",
    "            if len(top2) != 0:\n",
    "                for place in top2:\n",
    "                    if place == \"NoneType\": # no countries mentioned\n",
    "                        pass\n",
    "                    country = country_from_city(place)\n",
    "\n",
    "                    if country not in countries_mentioned: #don't double add\n",
    "                        countries_mentioned.append(country)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        bbc_df.at[index,\"Final Countries\"] = countries_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Same as above, but for Reuter.\n",
    "Extract Countries for all tweet based on the text or from the article in the hyperlink or the tweet text\"\"\"\n",
    "\n",
    "directory =  \"/Users/jennychan/Desktop/CS 234 Final Project/Reuters/\"\n",
    "\n",
    "for index in range(0,len(reu_df)) :\n",
    "    tweet_countries = reu_df.at[index,\"Countries from text\"]\n",
    "    \n",
    "    if pd.isna(tweet_countries): #if no countries\n",
    "        tweet_countries = \"[]\"\n",
    "    \n",
    "    #tweet_countries = tweet_countries.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "  #  lis = tweet_countries.split(\"',\")\n",
    "  #  final_countries_list = []\n",
    "  #  for i in lis:\n",
    "   #     i = i.strip().replace(\"'\",\"\")\n",
    "     #   final_countries_list.append(i)\n",
    "    \n",
    "    if len(final_countries_list) != 0: # if tweet text contains countries already\n",
    "        countries_mentioned = []\n",
    "        for place in final_countries_list:\n",
    "            country = country_from_city(place)\n",
    "    \n",
    "            if country not in countries_mentioned: #don't double add\n",
    "                countries_mentioned.append(country)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        reu_df.at[index,\"Final Countries\"] = countries_mentioned\n",
    "    \n",
    "    else: #if they didn't mention countries in the text, open txt file\n",
    "        tweetID = bbc_df.at[index,\"id\"]\n",
    "        with open('{}{}.txt'.format(bbc_directory, tweetID)) as f:\n",
    "            lines = f.read()\n",
    "            all_places = find_places(bbc_df)\n",
    "            top2 = top_two_places(all_places)\n",
    "            countries_mentioned = []\n",
    "            \n",
    "            if len(top2) != 0:\n",
    "                for place in top2:\n",
    "                    if place == \"NoneType\":\n",
    "                        pass\n",
    "                    \n",
    "                    country = country_from_city(place)\n",
    "\n",
    "                    if country not in countries_mentioned: #don't double add\n",
    "                        countries_mentioned.append(country)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        reu_df.at[index,\"Final Countries\"] = countries_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df.to_csv(\"CNN Breaking News.csv\") #save df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8da2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df.to_csv(\"BBC Breaking News.csv\") #save df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_df.to_csv(\"Reuters.csv\") #save df to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1d4be",
   "metadata": {},
   "source": [
    "## Extract the income level of the countries\n",
    "This part uses the world bank wbdata library based on the World Bank Data. It takes in the country and finds the income level according to the World Bank's metrics on GNI per capita of the country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function takes in the dataframe and finds all the unique countries mentioned in the 'Final Countries' column\"\"\"\n",
    "\n",
    "def countries_set(dataframe):\n",
    "    \n",
    "    countries_set = set()\n",
    "    for index in range(0, len(dataframe)):\n",
    "        final_countries = dataframe.at[index,\"Final Countries\"]\n",
    "        \n",
    "        #final_countries = final_countries.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        #lis = final_countries.split(\"',\")\n",
    "        #final_countries_list = []\n",
    "        #for i in lis:\n",
    "         #   i = i.strip().replace(\"'\",\"\")\n",
    "          #  final_countries_list.append(i)\n",
    "\n",
    "        #dataframe.at[index,\"Final Countries\"] = final_countries_list\n",
    "\n",
    "        for country in final_countries:\n",
    "            countries_set.add(country)      \n",
    "   # countries_set.remove()\n",
    "    return countries_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da2bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_set = countries_set(cnn_df)\n",
    "cnn_set.remove('')\n",
    "country_cnn = pd.DataFrame() #populate to new dataframe\n",
    "len(cnn_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_set = countries_set(bbc_df)\n",
    "country_bbc = pd.DataFrame()\n",
    "len(bbc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e167f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_set = countries_set(reu_df)\n",
    "len(reu_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8eefa08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes in the set of unique countries and the original dataframe from Twitter. \n",
    "For each country in the unique countries set, \n",
    "it finds the average number of responses of each post that mentioned the country, \n",
    "and returns a dictionary that has the country as the key, and the mean number as the value. \n",
    "\"\"\"\n",
    "def countries_reactions(countries_set, dataframe):\n",
    "    dic = dict()\n",
    "    for country in countries_set: \n",
    "        responses = 0\n",
    "        num_tweets = 0\n",
    "        for index in range(0,len(dataframe)):\n",
    "            countries = dataframe.at[index,\"Final Countries\"]\n",
    "            reactions = dataframe.at[index,\"# Reactions\"]\n",
    "\n",
    "            for country_mentioned in countries:\n",
    "                if country_mentioned == country:\n",
    "                    num_tweets += 1 #count the number of tweets that include this country\n",
    "                    responses += reactions #add the reactions to the total number of responses\n",
    "                    \n",
    "        dic[country] = round(responses/num_tweets,0) #find the mean\n",
    "        \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6298514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function takes in three parameters:\n",
    "1. countries_set - the set of unique countries from the dataframe\n",
    "2. df_name - create a new dataframe named df_name\n",
    "3. in_dataframe - the original dataframe that contains information for twitter (use to run the countries_reaction function)\n",
    "\"\"\"\n",
    "\n",
    "def countries_level(countries_set, df_name, in_dataframe):\n",
    "    coun_dic = countries_reactions(countries_set,in_dataframe) #run the function countries_reactions that returns a dictionary\n",
    "    df_name = pd.DataFrame(coun_dic.items(), columns=['Countries','# of Reactions']) # populate the dataframe with the dictionary\n",
    "    \n",
    "    for index in range(0,len(df_name)): #itterate through all countries in the dataframe\n",
    "        country = df_name.at[index, \"Countries\"]\n",
    "\n",
    "        try:\n",
    "            country_income = wbdata.search_countries(country)[0][\"incomeLevel\"][\"value\"] #extraact the country income level\n",
    "            df_name.at[index,\"Income Level\"] = country_income\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "            \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_level(reu_set, country_reu, reu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "312abefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_cnn = countries_level(cnn_set,country_cnn,cnn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_bbc = countries_level(bbc_set, country_bbc, bbc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_reu.to_csv(\"Reuters Countries.csv\") #save to a new dataframe use for other parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f11370",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_bbc.to_csv(\"BBC Countries.csv\") #save to a new dataframe use for other parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3c2d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_cnn.to_csv(\"CNN Countries.csv\") #save to a new dataframe use for other parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1977903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
